"""
MCP å·¥å…·æ¼”ç¤º - ä¸éœ€è¦ Claude API
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ MCP å·¥å…·è·å–å­¦æœ¯æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š
"""
import asyncio
import json
import sys
import datetime
from mcp_sdk import GiiispMCPClient

if sys.platform == "win32":
    try:
        sys.stdout.reconfigure(encoding="utf-8")
    except:
        pass

async def demo_mcp_tools():
    """æ¼”ç¤º MCP å·¥å…·çš„ä½¿ç”¨ï¼ˆä¸éœ€è¦ Claude APIï¼‰"""

    print("\n" + "="*80)
    print("ğŸ”¬ MCP å­¦æœ¯æœç´¢å·¥å…·æ¼”ç¤º")
    print("="*80)
    print("è¯´æ˜ï¼šæ­¤æ¼”ç¤ºå±•ç¤ºå¦‚ä½•ä½¿ç”¨ 6000-6007 ç«¯å£çš„ MCP æœåŠ¡è·å–å­¦æœ¯æ•°æ®")
    print("="*80)

    all_papers = []

    # 1. DeepResearch - ç»¼åˆæœç´¢
    print("\n[1/3] ğŸ” ä½¿ç”¨ DeepResearch æœç´¢ 'Large Language Models'...")
    client1 = GiiispMCPClient(6002, "DeepResearch")
    data1 = await client1.call_tool("DeepResearch", {"searchQuery": "Large Language Models", "count": 5})

    if data1:
        papers = data1.get("data", {}).get("data", [])
        for paper in papers:
            if isinstance(paper, dict):
                all_papers.append({
                    "source": "DeepResearch (é›†æ€è°±)",
                    "title": paper.get("title", "æœªçŸ¥æ ‡é¢˜"),
                    "authors": paper.get("authors", "æœªçŸ¥ä½œè€…"),
                    "year": paper.get("year", "æœªçŸ¥å¹´ä»½"),
                    "doi": paper.get("doi", ""),
                    "abstract": paper.get("abstractText", "æš‚æ— æ‘˜è¦")[:200] + "...",
                    "url": paper.get("link") or paper.get("doi") or "#"
                })
        print(f"   âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

    # 2. arXiv Abstract Search
    print("\n[2/3] ğŸ“š ä½¿ç”¨ arXiv æœç´¢ 'GPT' ç›¸å…³è®ºæ–‡...")
    client2 = GiiispMCPClient(6003, "Arxiv Abstract")
    data2 = await client2.call_tool("searchArxivByAbstract", {"key": "GPT", "pageSize": 5})

    if data2:
        papers = data2.get("data", {}).get("data", [])
        for paper in papers:
            if isinstance(paper, dict):
                arxiv_id = paper.get("arxivNo") or paper.get("arvixNo", "")
                if "arXiv:" in str(arxiv_id):
                    arxiv_id = arxiv_id.replace("arXiv:", "")

                all_papers.append({
                    "source": "arXiv (é¢„å°æœ¬)",
                    "title": paper.get("title", "æœªçŸ¥æ ‡é¢˜"),
                    "authors": paper.get("authors", "æœªçŸ¥ä½œè€…"),
                    "year": paper.get("year", "æœªçŸ¥å¹´ä»½"),
                    "arxiv_id": arxiv_id,
                    "abstract": paper.get("paperAbstract", "æš‚æ— æ‘˜è¦")[:200] + "...",
                    "url": f"https://arxiv.org/abs/{arxiv_id}" if arxiv_id else "#"
                })
        print(f"   âœ… æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

    # 3. Crossref - å­¦æœ¯æ–‡çŒ®å…ƒæ•°æ®
    print("\n[3/3] ğŸ“– ä½¿ç”¨ Crossref æœç´¢ 'Transformer' ç›¸å…³è®ºæ–‡...")
    client3 = GiiispMCPClient(6000, "Crossref")
    data3 = await client3.call_tool("search_works", {"query": "Transformer neural network", "rows": 5})

    if data3:
        items = data3.get("message", {}).get("items", [])
        for item in items:
            if isinstance(item, dict):
                all_papers.append({
                    "source": "Crossref (å…ƒæ•°æ®)",
                    "title": item.get("title", ["æœªçŸ¥æ ‡é¢˜"])[0] if item.get("title") else "æœªçŸ¥æ ‡é¢˜",
                    "authors": ", ".join([f"{a.get('given', '')} {a.get('family', '')}"
                                         for a in item.get("author", [])[:3]]),
                    "year": item.get("published", {}).get("date-parts", [[None]])[0][0],
                    "doi": item.get("DOI", ""),
                    "url": item.get("URL", "#")
                })
        print(f"   âœ… æ‰¾åˆ° {len(items)} ç¯‡è®ºæ–‡")

    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š æ•°æ®æ±‡æ€»")
    print("="*80)
    print(f"æ€»è®¡æ‰¾åˆ° {len(all_papers)} ç¯‡ç›¸å…³è®ºæ–‡\n")

    # ä¿å­˜ä¸º Markdown
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f"MCP_Demo_Report_{timestamp}.md"

    with open(filename, "w", encoding="utf-8") as f:
        f.write("# ğŸ“‘ Large Language Models å­¦æœ¯è°ƒç ”æŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
        f.write(f"**æ•°æ®æ¥æº**: DeepResearch, arXiv, Crossref\n\n")
        f.write(f"**è®ºæ–‡æ€»æ•°**: {len(all_papers)}\n\n")
        f.write("---\n\n")

        for idx, paper in enumerate(all_papers, 1):
            f.write(f"## {idx}. {paper['title']}\n\n")
            f.write(f"- **æ¥æº**: {paper['source']}\n")
            if paper.get('authors'):
                f.write(f"- **ä½œè€…**: {paper['authors']}\n")
            if paper.get('year'):
                f.write(f"- **å¹´ä»½**: {paper['year']}\n")
            if paper.get('doi'):
                f.write(f"- **DOI**: {paper['doi']}\n")
            if paper.get('arxiv_id'):
                f.write(f"- **arXiv ID**: {paper['arxiv_id']}\n")
            f.write(f"- **é“¾æ¥**: [æŸ¥çœ‹åŸæ–‡]({paper['url']})\n")
            if paper.get('abstract'):
                f.write(f"\n**æ‘˜è¦**:\n> {paper['abstract']}\n")
            f.write("\n---\n\n")

    print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {filename}")
    print("\n" + "="*80)
    print("ğŸ’¡ è¯´æ˜")
    print("="*80)
    print("æ­¤æ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ MCP å·¥å…·è·å–å­¦æœ¯æ•°æ®ã€‚")
    print("åœ¨å®é™…çš„ claude_agent.py ä¸­ï¼ŒClaude ä¼šï¼š")
    print("  1. è‡ªä¸»å†³å®šè°ƒç”¨å“ªäº›å·¥å…·")
    print("  2. è§‚å¯Ÿè¿”å›çš„æ•°æ®")
    print("  3. åŠ¨æ€è°ƒæ•´æœç´¢ç­–ç•¥")
    print("  4. æ™ºèƒ½æ•´åˆå¤šä¸ªæ•°æ®æº")
    print("  5. ç”Ÿæˆæ›´ä¸“ä¸šçš„ç»¼è¿°æŠ¥å‘Š")
    print("\nå¦‚æœä½ çš„ API è¿æ¥é—®é¢˜è§£å†³åï¼Œå¯ä»¥è¿è¡Œ claude_agent.py ä½“éªŒå®Œæ•´åŠŸèƒ½ã€‚")
    print("="*80)

if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    asyncio.run(demo_mcp_tools())
